{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNL8P7htnbGfHY0dSi69PZa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sJYNQwjVPabh","executionInfo":{"status":"ok","timestamp":1694100263527,"user_tz":300,"elapsed":47195,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}},"outputId":"d888bd96-d31d-4f27-8626-4ebfec873ef7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["import torch as tc"],"metadata":{"id":"30xkCaC4Qjv4","executionInfo":{"status":"ok","timestamp":1694100545654,"user_tz":300,"elapsed":5230,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["\n","# Declare x1 as a PyTorch tensor with value 1.0, float64 type, and tracking gradients.\n","x1 = tc.tensor(1, dtype=tc.float64, requires_grad=True)\n","\n","# Compute gradient of the function x1 ** 2 at x1=1 using autograd's 'grad' function.\n","# 'grad' takes the function to be differentiated (x1 ** 2) and the variable w.r.t. which to differentiate (x1).\n","# The function returns a tuple of gradient tensors, and [0] gets the actual tensor for the gradient w.r.t x1.\n","# In this specific case, the tuple has only one element.\n","g1 = tc.autograd.grad(x1 ** 2, x1)[0]\n","print('The grad of x**2 at x=1: ', g1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vq7Eog8uQiGi","executionInfo":{"status":"ok","timestamp":1694100551869,"user_tz":300,"elapsed":208,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}},"outputId":"01220aba-9d7e-42fa-fa86-7bcee9e092dd"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["The grad of x**2 at x=1:  tensor(2., dtype=torch.float64)\n"]}]},{"cell_type":"code","source":["# Using the 'backward' function to calculate gradients. The result is stored in x1.grad.\n","(x1 ** 2).backward()\n","print('The grad of x**2 at x=1: ', x1.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lexljJxyQiJM","executionInfo":{"status":"ok","timestamp":1694100605301,"user_tz":300,"elapsed":171,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}},"outputId":"fe4696bf-cb7a-460e-d631-3f0fc34a8804"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["The grad of x**2 at x=1:  tensor(2., dtype=torch.float64)\n"]}]},{"cell_type":"code","source":["# Declare x3 as a tensor with elements [0, 1, 2], float64 type, and tracking gradients.\n","x3 = tc.arange(3, dtype=tc.float64, requires_grad=True)\n","\n","# Compute gradient of the function x3 ** 2 at 'each' element of x3, which is allowed by autograd.grad()\n","# 'grad_outputs' is a tensor of the same shape as x3 filled with ones. It weights the gradients during the sum reduction.\n","g3 = tc.autograd.grad(x3 ** 2, x3, grad_outputs=tc.ones_like(x3))[0]\n","\n","print('The grad of x**2 at x=\\n', x3.data, ':', g3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X0xlsn8cQiLj","executionInfo":{"status":"ok","timestamp":1694101211186,"user_tz":300,"elapsed":155,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}},"outputId":"70377926-c7bc-497e-e1f4-aec68c9b0956"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["The grad of x**2 at x=\n"," tensor([0., 1., 2.], dtype=torch.float64) : tensor([0., 2., 4.], dtype=torch.float64)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ajlwQfdKQiOB"},"execution_count":null,"outputs":[]}]}