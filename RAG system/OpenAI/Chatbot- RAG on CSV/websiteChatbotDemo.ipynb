{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMW1uwUcDLm+evgR66KdSYm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Acquiring data in text form is the first step to use embeddings. This tutorial creates a new set of data by crawling the OpenAI website, a technique that you can also use for your own company or personal website.\n","\n","link : https://platform.openai.com/docs/tutorials/web-qa-embeddings"],"metadata":{"id":"RGA9q8K8pj0v"}},{"cell_type":"markdown","source":["While this crawler is written from scratch, open source packages like Scrapy can also help with these operations."],"metadata":{"id":"EtVr30gspw_s"}},{"cell_type":"markdown","source":["Most of the code here have more specific comments for better comprehension."],"metadata":{"id":"JC-dkqRYWgr5"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MLTYOPazti3l","executionInfo":{"status":"ok","timestamp":1707664127183,"user_tz":360,"elapsed":30375,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}},"outputId":"277f04db-36d5-4ccb-d0a7-f5f36ab7e7b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["%cd gdrive/MyDrive/Colab Notebooks/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CC7ZkhpFtv60","executionInfo":{"status":"ok","timestamp":1707664149671,"user_tz":360,"elapsed":337,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}},"outputId":"aefa69e3-6723-4fa2-af6c-915d68fff114"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/Colab Notebooks\n"]}]},{"cell_type":"code","source":["!pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"UeT4nEYZpjAl","executionInfo":{"status":"ok","timestamp":1707664307211,"user_tz":360,"elapsed":155262,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}},"outputId":"d5fa4655-6bf9-4786-e211-ce6735ff5d68"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting aiohttp==3.8.5 (from -r requirements.txt (line 1))\n","  Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m0.8/1.0 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiosignal==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.3.1)\n","Collecting appnope==0.1.3 (from -r requirements.txt (line 3))\n","  Downloading appnope-0.1.3-py2.py3-none-any.whl (4.4 kB)\n","Collecting asttokens==2.2.1 (from -r requirements.txt (line 4))\n","  Downloading asttokens-2.2.1-py2.py3-none-any.whl (26 kB)\n","Collecting async-timeout==4.0.2 (from -r requirements.txt (line 5))\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting attrs==22.2.0 (from -r requirements.txt (line 6))\n","  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: backcall==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.2.0)\n","Collecting beautifulsoup4==4.11.1 (from -r requirements.txt (line 8))\n","  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting blobfile==2.0.1 (from -r requirements.txt (line 9))\n","  Downloading blobfile-2.0.1-py3-none-any.whl (73 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bs4==0.0.1 (from -r requirements.txt (line 10))\n","  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting certifi==2023.7.22 (from -r requirements.txt (line 11))\n","  Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting charset-normalizer==2.1.1 (from -r requirements.txt (line 12))\n","  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n","Collecting comm==0.1.2 (from -r requirements.txt (line 13))\n","  Downloading comm-0.1.2-py3-none-any.whl (6.5 kB)\n","Collecting contourpy==1.0.7 (from -r requirements.txt (line 14))\n","  Downloading contourpy-1.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.3/300.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cycler==0.11.0 (from -r requirements.txt (line 15))\n","  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n","Collecting debugpy==1.6.5 (from -r requirements.txt (line 16))\n","  Downloading debugpy-1.6.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting decorator==5.1.1 (from -r requirements.txt (line 17))\n","  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n","Collecting docopt==0.6.2 (from -r requirements.txt (line 18))\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: entrypoints==0.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (0.4)\n","Collecting executing==1.2.0 (from -r requirements.txt (line 20))\n","  Downloading executing-1.2.0-py2.py3-none-any.whl (24 kB)\n","Collecting filelock==3.9.0 (from -r requirements.txt (line 21))\n","  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\n","Collecting fonttools==4.38.0 (from -r requirements.txt (line 22))\n","  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist==1.3.3 (from -r requirements.txt (line 23))\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 24)) (0.20.3)\n","Collecting idna==3.4 (from -r requirements.txt (line 25))\n","  Downloading idna-3.4-py3-none-any.whl (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ipykernel==6.20.1 (from -r requirements.txt (line 26))\n","  Downloading ipykernel-6.20.1-py3-none-any.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.2/149.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ipython==8.10.0 (from -r requirements.txt (line 27))\n","  Downloading ipython-8.10.0-py3-none-any.whl (784 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m784.3/784.3 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jedi==0.18.2 (from -r requirements.txt (line 28))\n","  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting joblib==1.2.0 (from -r requirements.txt (line 29))\n","  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jupyter_client==7.4.8 (from -r requirements.txt (line 30))\n","  Downloading jupyter_client-7.4.8-py3-none-any.whl (133 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jupyter_core==5.1.3 (from -r requirements.txt (line 31))\n","  Downloading jupyter_core-5.1.3-py3-none-any.whl (93 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting kiwisolver==1.4.4 (from -r requirements.txt (line 32))\n","  Downloading kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting lxml==4.9.2 (from -r requirements.txt (line 33))\n","  Downloading lxml-4.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting matplotlib==3.6.3 (from -r requirements.txt (line 34))\n","  Downloading matplotlib-3.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib-inline==0.1.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 35)) (0.1.6)\n","Collecting multidict==6.0.4 (from -r requirements.txt (line 36))\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nest-asyncio==1.5.6 (from -r requirements.txt (line 37))\n","  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n","Collecting numpy==1.24.1 (from -r requirements.txt (line 38))\n","  Downloading numpy-1.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting openai==0.26.1 (from -r requirements.txt (line 39))\n","  Downloading openai-0.26.1.tar.gz (55 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting packaging==23.0 (from -r requirements.txt (line 40))\n","  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pandas==1.5.2 (from -r requirements.txt (line 41))\n","  Downloading pandas-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: parso==0.8.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 42)) (0.8.3)\n","Collecting pexpect==4.8.0 (from -r requirements.txt (line 43))\n","  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 44)) (0.7.5)\n","Requirement already satisfied: Pillow==9.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 45)) (9.4.0)\n","Collecting pipreqs==0.4.12 (from -r requirements.txt (line 46))\n","  Downloading pipreqs-0.4.12-py2.py3-none-any.whl (32 kB)\n","Collecting platformdirs==2.6.2 (from -r requirements.txt (line 47))\n","  Downloading platformdirs-2.6.2-py3-none-any.whl (14 kB)\n","Collecting plotly==5.12.0 (from -r requirements.txt (line 48))\n","  Downloading plotly-5.12.0-py2.py3-none-any.whl (15.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting prompt-toolkit==3.0.36 (from -r requirements.txt (line 49))\n","  Downloading prompt_toolkit-3.0.36-py3-none-any.whl (386 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.4/386.4 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting psutil==5.9.4 (from -r requirements.txt (line 50))\n","  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.2/280.2 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: ptyprocess==0.7.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 51)) (0.7.0)\n","Collecting pure-eval==0.2.2 (from -r requirements.txt (line 52))\n","  Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n","Collecting pycryptodomex==3.17 (from -r requirements.txt (line 53))\n","  Downloading pycryptodomex-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Pygments==2.15.0 (from -r requirements.txt (line 54))\n","  Downloading Pygments-2.15.0-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyparsing==3.0.9 (from -r requirements.txt (line 55))\n","  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 56)) (2.8.2)\n","Collecting pytz==2022.7.1 (from -r requirements.txt (line 57))\n","  Downloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting PyYAML==6.0 (from -r requirements.txt (line 58))\n","  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyzmq==24.0.1 (from -r requirements.txt (line 59))\n","  Downloading pyzmq-24.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting regex==2022.10.31 (from -r requirements.txt (line 60))\n","  Downloading regex-2022.10.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.5/770.5 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 61)) (2.31.0)\n","Collecting scikit-learn==1.2.0 (from -r requirements.txt (line 62))\n","  Downloading scikit_learn-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scipy==1.10.0 (from -r requirements.txt (line 63))\n","  Downloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 64)) (1.16.0)\n","Collecting soupsieve==2.3.2.post1 (from -r requirements.txt (line 65))\n","  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n","Collecting stack-data==0.6.2 (from -r requirements.txt (line 66))\n","  Downloading stack_data-0.6.2-py3-none-any.whl (24 kB)\n","Collecting tenacity==8.1.0 (from -r requirements.txt (line 67))\n","  Downloading tenacity-8.1.0-py3-none-any.whl (23 kB)\n","Collecting threadpoolctl==3.1.0 (from -r requirements.txt (line 68))\n","  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n","Collecting tiktoken==0.1.2 (from -r requirements.txt (line 69))\n","  Downloading tiktoken-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers==0.13.2 (from -r requirements.txt (line 70))\n","  Downloading tokenizers-0.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tornado==6.3.3 (from -r requirements.txt (line 71))\n","  Downloading tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.7/427.7 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tqdm==4.64.1 (from -r requirements.txt (line 72))\n","  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting traitlets==5.8.1 (from -r requirements.txt (line 73))\n","  Downloading traitlets-5.8.1-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.8/116.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting transformers==4.30.0 (from -r requirements.txt (line 74))\n","  Downloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing_extensions==4.4.0 (from -r requirements.txt (line 75))\n","  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n","Collecting urllib3==1.26.13 (from -r requirements.txt (line 76))\n","  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting wcwidth==0.2.5 (from -r requirements.txt (line 77))\n","  Downloading wcwidth-0.2.5-py2.py3-none-any.whl (30 kB)\n","Collecting yarg==0.1.9 (from -r requirements.txt (line 78))\n","  Downloading yarg-0.1.9-py2.py3-none-any.whl (19 kB)\n","Collecting yarl==1.8.2 (from -r requirements.txt (line 79))\n","  Downloading yarl-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.0/264.0 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0->-r requirements.txt (line 74)) (0.4.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.12->-r requirements.txt (line 24)) (2023.6.0)\n","Building wheels for collected packages: bs4, docopt, openai\n","  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1256 sha256=534a6a7c929e3e8b4dd5b9b25fd20c37c1e249fe93040830e7f2c826aa2f99ef\n","  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=9092219747b7342550c02011f7177b760addd66ce4b4887d692fbf3d5b612dfb\n","  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n","  Building wheel for openai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai: filename=openai-0.26.1-py3-none-any.whl size=67285 sha256=593cab68474ee9e5ac960b58d24775095a25647fd04ce2eba2c5dfe85323a427\n","  Stored in directory: /root/.cache/pip/wheels/18/cc/20/ba0ee3af9dddcdaa3496652087be11b09f28956640528bd7c6\n","Successfully built bs4 docopt openai\n","Installing collected packages: wcwidth, tokenizers, pytz, pure-eval, executing, docopt, appnope, urllib3, typing_extensions, traitlets, tqdm, tornado, threadpoolctl, tenacity, soupsieve, regex, pyzmq, PyYAML, pyparsing, Pygments, pycryptodomex, psutil, prompt-toolkit, platformdirs, pexpect, packaging, numpy, nest-asyncio, multidict, lxml, kiwisolver, joblib, jedi, idna, frozenlist, fonttools, filelock, decorator, debugpy, cycler, charset-normalizer, certifi, attrs, async-timeout, asttokens, yarl, stack-data, scipy, plotly, pandas, jupyter_core, contourpy, comm, blobfile, beautifulsoup4, yarg, tiktoken, scikit-learn, matplotlib, jupyter_client, ipython, bs4, aiohttp, transformers, pipreqs, openai, ipykernel\n","  Attempting uninstall: wcwidth\n","    Found existing installation: wcwidth 0.2.13\n","    Uninstalling wcwidth-0.2.13:\n","      Successfully uninstalled wcwidth-0.2.13\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.15.1\n","    Uninstalling tokenizers-0.15.1:\n","      Successfully uninstalled tokenizers-0.15.1\n","  Attempting uninstall: pytz\n","    Found existing installation: pytz 2023.4\n","    Uninstalling pytz-2023.4:\n","      Successfully uninstalled pytz-2023.4\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 2.0.7\n","    Uninstalling urllib3-2.0.7:\n","      Successfully uninstalled urllib3-2.0.7\n","  Attempting uninstall: typing_extensions\n","    Found existing installation: typing_extensions 4.9.0\n","    Uninstalling typing_extensions-4.9.0:\n","      Successfully uninstalled typing_extensions-4.9.0\n","  Attempting uninstall: traitlets\n","    Found existing installation: traitlets 5.7.1\n","    Uninstalling traitlets-5.7.1:\n","      Successfully uninstalled traitlets-5.7.1\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.66.1\n","    Uninstalling tqdm-4.66.1:\n","      Successfully uninstalled tqdm-4.66.1\n","  Attempting uninstall: tornado\n","    Found existing installation: tornado 6.3.2\n","    Uninstalling tornado-6.3.2:\n","      Successfully uninstalled tornado-6.3.2\n","  Attempting uninstall: threadpoolctl\n","    Found existing installation: threadpoolctl 3.2.0\n","    Uninstalling threadpoolctl-3.2.0:\n","      Successfully uninstalled threadpoolctl-3.2.0\n","  Attempting uninstall: tenacity\n","    Found existing installation: tenacity 8.2.3\n","    Uninstalling tenacity-8.2.3:\n","      Successfully uninstalled tenacity-8.2.3\n","  Attempting uninstall: soupsieve\n","    Found existing installation: soupsieve 2.5\n","    Uninstalling soupsieve-2.5:\n","      Successfully uninstalled soupsieve-2.5\n","  Attempting uninstall: regex\n","    Found existing installation: regex 2023.12.25\n","    Uninstalling regex-2023.12.25:\n","      Successfully uninstalled regex-2023.12.25\n","  Attempting uninstall: pyzmq\n","    Found existing installation: pyzmq 23.2.1\n","    Uninstalling pyzmq-23.2.1:\n","      Successfully uninstalled pyzmq-23.2.1\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 6.0.1\n","    Uninstalling PyYAML-6.0.1:\n","      Successfully uninstalled PyYAML-6.0.1\n","  Attempting uninstall: pyparsing\n","    Found existing installation: pyparsing 3.1.1\n","    Uninstalling pyparsing-3.1.1:\n","      Successfully uninstalled pyparsing-3.1.1\n","  Attempting uninstall: Pygments\n","    Found existing installation: Pygments 2.16.1\n","    Uninstalling Pygments-2.16.1:\n","      Successfully uninstalled Pygments-2.16.1\n","  Attempting uninstall: psutil\n","    Found existing installation: psutil 5.9.5\n","    Uninstalling psutil-5.9.5:\n","      Successfully uninstalled psutil-5.9.5\n","  Attempting uninstall: prompt-toolkit\n","    Found existing installation: prompt-toolkit 3.0.43\n","    Uninstalling prompt-toolkit-3.0.43:\n","      Successfully uninstalled prompt-toolkit-3.0.43\n","  Attempting uninstall: platformdirs\n","    Found existing installation: platformdirs 4.2.0\n","    Uninstalling platformdirs-4.2.0:\n","      Successfully uninstalled platformdirs-4.2.0\n","  Attempting uninstall: pexpect\n","    Found existing installation: pexpect 4.9.0\n","    Uninstalling pexpect-4.9.0:\n","      Successfully uninstalled pexpect-4.9.0\n","  Attempting uninstall: packaging\n","    Found existing installation: packaging 23.2\n","    Uninstalling packaging-23.2:\n","      Successfully uninstalled packaging-23.2\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.23.5\n","    Uninstalling numpy-1.23.5:\n","      Successfully uninstalled numpy-1.23.5\n","  Attempting uninstall: nest-asyncio\n","    Found existing installation: nest-asyncio 1.6.0\n","    Uninstalling nest-asyncio-1.6.0:\n","      Successfully uninstalled nest-asyncio-1.6.0\n","  Attempting uninstall: multidict\n","    Found existing installation: multidict 6.0.5\n","    Uninstalling multidict-6.0.5:\n","      Successfully uninstalled multidict-6.0.5\n","  Attempting uninstall: lxml\n","    Found existing installation: lxml 4.9.4\n","    Uninstalling lxml-4.9.4:\n","      Successfully uninstalled lxml-4.9.4\n","  Attempting uninstall: kiwisolver\n","    Found existing installation: kiwisolver 1.4.5\n","    Uninstalling kiwisolver-1.4.5:\n","      Successfully uninstalled kiwisolver-1.4.5\n","  Attempting uninstall: joblib\n","    Found existing installation: joblib 1.3.2\n","    Uninstalling joblib-1.3.2:\n","      Successfully uninstalled joblib-1.3.2\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.6\n","    Uninstalling idna-3.6:\n","      Successfully uninstalled idna-3.6\n","  Attempting uninstall: frozenlist\n","    Found existing installation: frozenlist 1.4.1\n","    Uninstalling frozenlist-1.4.1:\n","      Successfully uninstalled frozenlist-1.4.1\n","  Attempting uninstall: fonttools\n","    Found existing installation: fonttools 4.48.1\n","    Uninstalling fonttools-4.48.1:\n","      Successfully uninstalled fonttools-4.48.1\n","  Attempting uninstall: filelock\n","    Found existing installation: filelock 3.13.1\n","    Uninstalling filelock-3.13.1:\n","      Successfully uninstalled filelock-3.13.1\n","  Attempting uninstall: decorator\n","    Found existing installation: decorator 4.4.2\n","    Uninstalling decorator-4.4.2:\n","      Successfully uninstalled decorator-4.4.2\n","  Attempting uninstall: debugpy\n","    Found existing installation: debugpy 1.6.6\n","    Uninstalling debugpy-1.6.6:\n","      Successfully uninstalled debugpy-1.6.6\n","  Attempting uninstall: cycler\n","    Found existing installation: cycler 0.12.1\n","    Uninstalling cycler-0.12.1:\n","      Successfully uninstalled cycler-0.12.1\n","  Attempting uninstall: charset-normalizer\n","    Found existing installation: charset-normalizer 3.3.2\n","    Uninstalling charset-normalizer-3.3.2:\n","      Successfully uninstalled charset-normalizer-3.3.2\n","  Attempting uninstall: certifi\n","    Found existing installation: certifi 2024.2.2\n","    Uninstalling certifi-2024.2.2:\n","      Successfully uninstalled certifi-2024.2.2\n","  Attempting uninstall: attrs\n","    Found existing installation: attrs 23.2.0\n","    Uninstalling attrs-23.2.0:\n","      Successfully uninstalled attrs-23.2.0\n","  Attempting uninstall: async-timeout\n","    Found existing installation: async-timeout 4.0.3\n","    Uninstalling async-timeout-4.0.3:\n","      Successfully uninstalled async-timeout-4.0.3\n","  Attempting uninstall: yarl\n","    Found existing installation: yarl 1.9.4\n","    Uninstalling yarl-1.9.4:\n","      Successfully uninstalled yarl-1.9.4\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.11.4\n","    Uninstalling scipy-1.11.4:\n","      Successfully uninstalled scipy-1.11.4\n","  Attempting uninstall: plotly\n","    Found existing installation: plotly 5.15.0\n","    Uninstalling plotly-5.15.0:\n","      Successfully uninstalled plotly-5.15.0\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 1.5.3\n","    Uninstalling pandas-1.5.3:\n","      Successfully uninstalled pandas-1.5.3\n","  Attempting uninstall: jupyter_core\n","    Found existing installation: jupyter_core 5.7.1\n","    Uninstalling jupyter_core-5.7.1:\n","      Successfully uninstalled jupyter_core-5.7.1\n","  Attempting uninstall: contourpy\n","    Found existing installation: contourpy 1.2.0\n","    Uninstalling contourpy-1.2.0:\n","      Successfully uninstalled contourpy-1.2.0\n","  Attempting uninstall: beautifulsoup4\n","    Found existing installation: beautifulsoup4 4.12.3\n","    Uninstalling beautifulsoup4-4.12.3:\n","      Successfully uninstalled beautifulsoup4-4.12.3\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.2.2\n","    Uninstalling scikit-learn-1.2.2:\n","      Successfully uninstalled scikit-learn-1.2.2\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.7.1\n","    Uninstalling matplotlib-3.7.1:\n","      Successfully uninstalled matplotlib-3.7.1\n","  Attempting uninstall: jupyter_client\n","    Found existing installation: jupyter-client 6.1.12\n","    Uninstalling jupyter-client-6.1.12:\n","      Successfully uninstalled jupyter-client-6.1.12\n","  Attempting uninstall: ipython\n","    Found existing installation: ipython 7.34.0\n","    Uninstalling ipython-7.34.0:\n","      Successfully uninstalled ipython-7.34.0\n","  Attempting uninstall: aiohttp\n","    Found existing installation: aiohttp 3.9.3\n","    Uninstalling aiohttp-3.9.3:\n","      Successfully uninstalled aiohttp-3.9.3\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.35.2\n","    Uninstalling transformers-4.35.2:\n","      Successfully uninstalled transformers-4.35.2\n","  Attempting uninstall: ipykernel\n","    Found existing installation: ipykernel 5.5.6\n","    Uninstalling ipykernel-5.5.6:\n","      Successfully uninstalled ipykernel-5.5.6\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","sqlalchemy 2.0.25 requires typing-extensions>=4.6.0, but you have typing-extensions 4.4.0 which is incompatible.\n","bigframes 0.20.1 requires scikit-learn>=1.2.2, but you have scikit-learn 1.2.0 which is incompatible.\n","google-colab 1.0.0 requires ipykernel==5.5.6, but you have ipykernel 6.20.1 which is incompatible.\n","google-colab 1.0.0 requires ipython==7.34.0, but you have ipython 8.10.0 which is incompatible.\n","google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 1.5.2 which is incompatible.\n","google-colab 1.0.0 requires tornado==6.3.2, but you have tornado 6.3.3 which is incompatible.\n","moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\n","pydantic 2.6.1 requires typing-extensions>=4.6.1, but you have typing-extensions 4.4.0 which is incompatible.\n","pydantic-core 2.16.2 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.4.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed PyYAML-6.0 Pygments-2.15.0 aiohttp-3.8.5 appnope-0.1.3 asttokens-2.2.1 async-timeout-4.0.2 attrs-22.2.0 beautifulsoup4-4.11.1 blobfile-2.0.1 bs4-0.0.1 certifi-2023.7.22 charset-normalizer-2.1.1 comm-0.1.2 contourpy-1.0.7 cycler-0.11.0 debugpy-1.6.5 decorator-5.1.1 docopt-0.6.2 executing-1.2.0 filelock-3.9.0 fonttools-4.38.0 frozenlist-1.3.3 idna-3.4 ipykernel-6.20.1 ipython-8.10.0 jedi-0.18.2 joblib-1.2.0 jupyter_client-7.4.8 jupyter_core-5.1.3 kiwisolver-1.4.4 lxml-4.9.2 matplotlib-3.6.3 multidict-6.0.4 nest-asyncio-1.5.6 numpy-1.24.1 openai-0.26.1 packaging-23.0 pandas-1.5.2 pexpect-4.8.0 pipreqs-0.4.12 platformdirs-2.6.2 plotly-5.12.0 prompt-toolkit-3.0.36 psutil-5.9.4 pure-eval-0.2.2 pycryptodomex-3.17 pyparsing-3.0.9 pytz-2022.7.1 pyzmq-24.0.1 regex-2022.10.31 scikit-learn-1.2.0 scipy-1.10.0 soupsieve-2.3.2.post1 stack-data-0.6.2 tenacity-8.1.0 threadpoolctl-3.1.0 tiktoken-0.1.2 tokenizers-0.13.2 tornado-6.3.3 tqdm-4.64.1 traitlets-5.8.1 transformers-4.30.0 typing_extensions-4.4.0 urllib3-1.26.13 wcwidth-0.2.5 yarg-0.1.9 yarl-1.8.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["IPython","certifi","cycler","debugpy","decorator","kiwisolver","matplotlib","mpl_toolkits","numpy","pexpect","prompt_toolkit","psutil","pygments","tornado","wcwidth","zmq"]}}},"metadata":{}}]},{"cell_type":"code","source":["import requests          # For making HTTP requests to a website.\n","import re               # 're' is for regular expressions, used for text pattern matching.\n","import urllib.request   # For opening and reading URLs.\n","from bs4 import BeautifulSoup   # BeautifulSoup, from bs4, is used for parsing HTML and XML documents.\n","from collections import deque   # 'deque' is a list-like container with fast appends and pops.\n","from html.parser import HTMLParser  # For parsing HTML.\n","from urllib.parse import urlparse  # Parses URLs into components.\n","import os               # Provides a way of using operating system dependent functionality."],"metadata":{"id":"181Mzsftt4HR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Regex pattern to match a URL\n","HTTP_URL_PATTERN = r'^http[s]*://.+'   # Regular expression to match http and https URLs.\n","\n","domain = \"openai.com\" # Domain to be crawled.\n","full_url = \"https://openai.com/\" # Full URL of the domain, including http/https.\n","\n","# Create a class to parse the HTML and get the hyperlinks\n","class HyperlinkParser(HTMLParser):\n","    def __init__(self):\n","        super().__init__()  # Initialize the base class.\n","        self.hyperlinks = []  # List to store the hyperlinks found.\n","\n","    # Override the handle_starttag method of HTMLParser to extract hyperlinks.\n","    def handle_starttag(self, tag, attrs):\n","        attrs = dict(attrs)  # Convert attrs tuple to a dictionary.\n","\n","        # Check if the tag is an anchor tag ('a') and has an 'href' attribute.\n","        if tag == \"a\" and \"href\" in attrs:\n","            self.hyperlinks.append(attrs[\"href\"])  # Add the 'href' value to the hyperlinks list.\n"],"metadata":{"id":"J64XGN12vsQY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The next function takes a URL as an argument, opens the URL, and reads the HTML content. Then, it returns all the hyperlinks found on that page."],"metadata":{"id":"kCKarhjxU--k"}},{"cell_type":"code","source":["# Function to get the hyperlinks from a URL\n","def get_hyperlinks(url):\n","    try:\n","        # Open the URL and read the HTML\n","        with urllib.request.urlopen(url) as response:\n","            # Check if the content type is HTML, return empty if not\n","            if not response.info().get('Content-Type').startswith(\"text/html\"):\n","                return []\n","\n","            # Read and decode the HTML content\n","            html = response.read().decode('utf-8')\n","\n","    except Exception as e:  # Handle exceptions like connection errors\n","        print(e)\n","        return []\n","\n","    # Create an instance of the HyperlinkParser class\n","    parser = HyperlinkParser()\n","    parser.feed(html)  # Feed the HTML content to the parser\n","\n","    return parser.hyperlinks  # Return the list of hyperlinks extracted\n"],"metadata":{"id":"1c5Dl85jU_Uh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The goal is to crawl through and index only the content that lives under the OpenAI domain. For this purpose, a function that calls the get_hyperlinks function but filters out any URLs that are not part of the specified domain is needed."],"metadata":{"id":"SEEbqXdVVWJ7"}},{"cell_type":"code","source":["# Function to get hyperlinks from a URL within the same domain\n","def get_domain_hyperlinks(local_domain, url):\n","    clean_links = []  # List to store the cleaned and filtered links\n","    for link in set(get_hyperlinks(url)):  # Get hyperlinks and remove duplicates using set\n","        clean_link = None  # Initialize a variable to store the processed link\n","\n","        # Check if the link is an absolute URL and within the same domain\n","        if re.search(HTTP_URL_PATTERN, link):\n","            url_obj = urlparse(link)  # Parse the URL\n","            if url_obj.netloc == local_domain:  # Check if the domain matches\n","                clean_link = link\n","\n","        # Handle relative links\n","        else:\n","            if link.startswith(\"/\"):\n","                link = link[1:]  # Remove leading slash for consistency\n","            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n","                continue  # Skip anchor and mailto links\n","            clean_link = \"https://\" + local_domain + \"/\" + link\n","\n","        # Clean up the link and add to the list\n","        if clean_link is not None:\n","            if clean_link.endswith(\"/\"):\n","                clean_link = clean_link[:-1]  # Remove trailing slash\n","            clean_links.append(clean_link)\n","\n","    return list(set(clean_links))  # Return a list of unique links\n"],"metadata":{"id":"ZhZm49FgVWrN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The crawl function is the final step in the web scraping task setup. It keeps track of the visited URLs to avoid repeating the same page, which might be linked across multiple pages on a site. It also extracts the raw text from a page without the HTML tags, and writes the text content into a local .txt file specific to the page."],"metadata":{"id":"guK2GZBdVYct"}},{"cell_type":"code","source":["def crawl(url):\n","    # Extract the domain from the URL\n","    local_domain = urlparse(url).netloc\n","\n","    # Initialize a queue with the starting URL\n","    queue = deque([url])\n","\n","    # Set to keep track of visited URLs\n","    seen = set([url])\n","\n","    # Create directories for storing text and processed data\n","    os.makedirs(\"text/\", exist_ok=True)\n","    os.makedirs(f\"text/{local_domain}/\", exist_ok=True)\n","    os.makedirs(\"processed\", exist_ok=True)\n","\n","    # Start crawling\n","    while queue:\n","        current_url = queue.pop()\n","        print(current_url)  # Output for progress tracking\n","\n","        # Open a file for the current URL\n","        filename = 'text/' + local_domain + '/' + current_url[8:].replace(\"/\", \"_\") + \".txt\"\n","        with open(filename, \"w\", encoding=\"UTF-8\") as f:\n","            response = requests.get(current_url)\n","            soup = BeautifulSoup(response.text, \"html.parser\")\n","            text = soup.get_text()\n","\n","            # Check for JavaScript dependency\n","            if \"You need to enable JavaScript to run this app.\" in text:\n","                print(\"Unable to parse page \" + current_url)\n","\n","            # Write the text to the file\n","            f.write(text)\n","\n","        # Add new links to the queue\n","        for link in get_domain_hyperlinks(local_domain, current_url):\n","            if link not in seen:\n","                queue.appendleft(link)\n","                seen.add(link)\n","\n","# Start the crawling process\n","crawl(full_url)\n"],"metadata":{"id":"b5Xd7FbHVeGC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["CSV is a common format for storing embeddings. You can use this format with Python by converting the raw text files (which are in the text directory) into Pandas data frames. Pandas is a popular open source library that helps you work with tabular data (data stored in rows and columns).\n","Blank empty lines can clutter the text files and make them harder to process. A simple function can remove those lines and tidy up the files."],"metadata":{"id":"bVVFjpi5Vjtp"}},{"cell_type":"code","source":["def remove_newlines(serie):\n","    # Replace newline characters with a space\n","    serie = serie.str.replace('\\n', ' ')\n","    # Replace escaped newline characters with a space\n","    serie = serie.str.replace('\\\\n', ' ')\n","    # Replace double spaces with a single space\n","    serie = serie.str.replace('  ', ' ')\n","    # Repeat replacing double spaces to handle cases of multiple consecutive spaces\n","    serie = serie.str.replace('  ', ' ')\n","    return serie\n"],"metadata":{"id":"Of3f4l23Vj_0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","The provided script converts text files from a specific directory into a CSV file using Pandas, a powerful data manipulation library in Python. The code reads the text files, processes their content, and stores them in a Pandas DataFrame which is then written to a CSV file:"],"metadata":{"id":"6O_80WtCVtEu"}},{"cell_type":"code","source":["import pandas as pd\n","\n","texts = []  # List to store the text files' contents\n","\n","# Loop through the files in the specified directory\n","for file in os.listdir(\"text/\" + domain + \"/\"):\n","    with open(\"text/\" + domain + \"/\" + file, \"r\", encoding=\"UTF-8\") as f:\n","        text = f.read()\n","\n","        # Modify the file name and append it with the text to the list\n","        modified_fname = file[11:-4].replace('-', ' ').replace('_', ' ').replace('#update', '')\n","        texts.append((modified_fname, text))\n","\n","# Create a DataFrame from the list\n","df = pd.DataFrame(texts, columns=['fname', 'text'])\n","\n","# Process the text column to remove new lines and extra spaces\n","df['text'] = df.fname + \". \" + remove_newlines(df['text'])\n","\n","# Write the DataFrame to a CSV file\n","df.to_csv('processed/scraped.csv')\n","\n","# Display the first few rows of the DataFrame\n","df.head()\n"],"metadata":{"id":"ACyMiI36VuM_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---------------------- the same as tutorial ------------------------\n"],"metadata":{"id":"o81g2EpzV7qk"}},{"cell_type":"code","source":["import tiktoken\n","\n","# Load the cl100k_base tokenizer which is designed to work with the ada-002 model\n","tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n","\n","df = pd.read_csv('processed/scraped.csv', index_col=0)\n","df.columns = ['title', 'text']\n","\n","# Tokenize the text and save the number of tokens to a new column\n","df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n","\n","# Visualize the distribution of the number of tokens per row using a histogram\n","df.n_tokens.hist()"],"metadata":{"id":"1fVtoX61V-0Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_tokens = 500\n","\n","# Function to split the text into chunks of a maximum number of tokens\n","def split_into_many(text, max_tokens = max_tokens):\n","\n","    # Split the text into sentences\n","    sentences = text.split('. ')\n","\n","    # Get the number of tokens for each sentence\n","    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n","\n","    chunks = []\n","    tokens_so_far = 0\n","    chunk = []\n","\n","    # Loop through the sentences and tokens joined together in a tuple\n","    for sentence, token in zip(sentences, n_tokens):\n","\n","        # If the number of tokens so far plus the number of tokens in the current sentence is greater\n","        # than the max number of tokens, then add the chunk to the list of chunks and reset\n","        # the chunk and tokens so far\n","        if tokens_so_far + token > max_tokens:\n","            chunks.append(\". \".join(chunk) + \".\")\n","            chunk = []\n","            tokens_so_far = 0\n","\n","        # If the number of tokens in the current sentence is greater than the max number of\n","        # tokens, go to the next sentence\n","        if token > max_tokens:\n","            continue\n","\n","        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n","        chunk.append(sentence)\n","        tokens_so_far += token + 1\n","\n","    return chunks\n","\n","\n","shortened = []\n","\n","# Loop through the dataframe\n","for row in df.iterrows():\n","\n","    # If the text is None, go to the next row\n","    if row[1]['text'] is None:\n","        continue\n","\n","    # If the number of tokens is greater than the max number of tokens, split the text into chunks\n","    if row[1]['n_tokens'] > max_tokens:\n","        shortened += split_into_many(row[1]['text'])\n","\n","    # Otherwise, add the text to the list of shortened texts\n","    else:\n","        shortened.append( row[1]['text'] )"],"metadata":{"id":"cMCE5SHtWCAN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.DataFrame(shortened, columns = ['text'])\n","df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n","df.n_tokens.hist()"],"metadata":{"id":"1pak_4y2WDs1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from openai import OpenAI\n","\n","client = OpenAI(\n","    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",")\n","\n","df['embeddings'] = df.text.apply(lambda x: client.embeddings.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding'])\n","\n","df.to_csv('processed/embeddings.csv')\n","df.head()"],"metadata":{"id":"rsGQ1kwpWFm6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from openai.embeddings_utils import distances_from_embeddings\n","\n","df=pd.read_csv('processed/embeddings.csv', index_col=0)\n","df['embeddings'] = df['embeddings'].apply(eval).apply(np.array)\n","\n","df.head()"],"metadata":{"id":"-D3-m9TPWLUI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_context(\n","    question, df, max_len=1800, size=\"ada\"\n","):\n","    \"\"\"\n","    Create a context for a question by finding the most similar context from the dataframe\n","    \"\"\"\n","\n","    # Get the embeddings for the question\n","    q_embeddings = client.embeddings.create(input=question, engine='text-embedding-ada-002')['data'][0]['embedding']\n","\n","    # Get the distances from the embeddings\n","    df['distances'] = distances_from_embeddings(q_embeddings, df['embeddings'].values, distance_metric='cosine')\n","\n","\n","    returns = []\n","    cur_len = 0\n","\n","    # Sort by distance and add the text to the context until the context is too long\n","    for i, row in df.sort_values('distances', ascending=True).iterrows():\n","\n","        # Add the length of the text to the current length\n","        cur_len += row['n_tokens'] + 4\n","\n","        # If the context is too long, break\n","        if cur_len > max_len:\n","            break\n","\n","        # Else add it to the text that is being returned\n","        returns.append(row[\"text\"])\n","\n","    # Return the context\n","    return \"\\n\\n###\\n\\n\".join(returns)"],"metadata":{"id":"YyZrxh5BWRlC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def answer_question(\n","    df,\n","    model=\"gpt-3.5-turbo\",\n","    question=\"Am I allowed to publish model outputs to Twitter, without a human review?\",\n","    max_len=1800,\n","    size=\"ada\",\n","    debug=False,\n","    max_tokens=150,\n","    stop_sequence=None\n","):\n","    \"\"\"\n","    Answer a question based on the most similar context from the dataframe texts\n","    \"\"\"\n","    context = create_context(\n","        question,\n","        df,\n","        max_len=max_len,\n","        size=size,\n","    )\n","    # If debug, print the raw model response\n","    if debug:\n","        print(\"Context:\\n\" + context)\n","        print(\"\\n\\n\")\n","\n","    try:\n","        # Create a chat completion using the question and context\n","        response = client.chat.completions.create(\n","            model=\"gpt-3.5-turbo\",\n","            messages=[\n","                {\"role\": \"system\", \"content\": \"Answer the question based on the context below, and if the question can't be answered based on the context, say \\\"I don't know\\\"\\n\\n\"},\n","                {\"role\": \"user\", f\"content\": \"Context: {context}\\n\\n---\\n\\nQuestion: {question}\\nAnswer:\"}\n","            ],\n","            temperature=0,\n","            max_tokens=max_tokens,\n","            top_p=1,\n","            frequency_penalty=0,\n","            presence_penalty=0,\n","            stop=stop_sequence,\n","        )\n","        return response.choices[0].message.strip()\n","    except Exception as e:\n","        print(e)\n","        return \"\""],"metadata":{"id":"8z03Bp-7WU5s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["answer_question(df, question=\"What day is it?\", debug=False)\n","\n","answer_question(df, question=\"What is our newest embeddings model?\")\n","\n","answer_question(df, question=\"What is ChatGPT?\")\n","\n","# \"I don't know.\"\n","\n","# 'The newest embeddings model is text-embedding-ada-002.'\n","\n","# 'ChatGPT is a model trained to interact in a conversational way. It is able to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.'"],"metadata":{"id":"sllK0eE7WWdi"},"execution_count":null,"outputs":[]}]}