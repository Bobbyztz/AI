{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNA7lcuXFgB/7sY6w4maO89"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"CIO8QXkgxAfA"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","from datasets import load_dataset\n","from openai import OpenAI\n","from pinecone import Pinecone, ServerlessSpec\n","from tqdm.auto import tqdm\n","from DLAIUtils import Utils\n","\n","import ast\n","import os\n","import pandas as pd\n","\n","# get api key\n","utils = Utils()\n","PINECONE_API_KEY = utils.get_pinecone_api_key()\n","\n","pinecone = Pinecone(api_key=PINECONE_API_KEY)\n","\n","utils = Utils()\n","INDEX_NAME = utils.create_dlai_index_name('dl-ai')\n","if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:\n","  pinecone.delete_index(INDEX_NAME)\n","\n","pinecone.create_index(name=INDEX_NAME, dimension=1536, metric='cosine',\n","  spec=ServerlessSpec(cloud='aws', region='us-west-2'))\n","\n","index = pinecone.Index(INDEX_NAME)"]},{"cell_type":"code","source":["#downloading and unzipping a dataset.\n","\n","#wget is a command-line utility for downloading files from the web.\n","#-q stands for \"quiet\" and tells wget to download the file without displaying progress or messages.\n","#-O lesson2-wiki.csv.zip specifies the output filename. It means the downloaded file will be saved as lesson2-wiki.csv.zip.\n","#The URL following -O is the direct link to the dataset hosted on Dropbox.\n","\n","#!wget -q -O lesson2-wiki.csv.zip \"https://www.dropbox.com/scl/fi/yxzmsrv2sgl249zcspeqb/lesson2-wiki.csv.zip?rlkey=paehnoxjl3s5x53d1bedt4pmc&dl=0\"\n","\n","#!unzip lesson2-wiki.csv.zip\n"],"metadata":{"id":"VUL2gfIixo8h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(Note: max_articles_num = 500): To achieve a more comprehensive context for the Language Learning Model, a larger number of articles is generally more beneficial. In this lab, we've initially set max_articles_num to 500 for speedier results, allowing you to observe the outcomes faster. Once you've done an initial run, consider increasing this value to 750 or 1,000. You'll likely notice that the context provided to the LLM becomes richer and better. You can experiment by gradually raising this variable for different queries to observe the improvements in the LLM's contextual understanding."],"metadata":{"id":"EK3l0Myay3vj"}},{"cell_type":"code","source":["max_articles_num = 500\n","df = pd.read_csv('./data/wiki.csv', nrows=max_articles_num)\n","df.head()\n"],"metadata":{"id":"ssDXVHfHy5EL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Unlike literal_eval, it only allows certain simple literal structures: strings, bytes, numbers, tuples, lists, dicts, sets, booleans, and None.\n","# import ast\n","\n","# # String representation of a list\n","# list_str = \"[1, 2, 3, 4, 5]\"\n","\n","# # Using ast.literal_eval to convert it back to a list\n","# converted_list = ast.literal_eval(list_str)\n","\n","# print(type(converted_list))  # This will show <class 'list'>\n","# print(converted_list)        # This will print [1, 2, 3, 4, 5]\n"],"metadata":{"id":"bK2H1hgJ2lw2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Prepare the Embeddings and Upsert to Pinecone\n","\n","#an empty list prepped to hold processed data before upserting it into Pinecone.\n","prepped = []\n","\n","# tqdm is used to display a progress bar, total=df.shape[0] indicating the total number of iterations.\n","for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n","    meta = ast.literal_eval(row['metadata'])\n","    prepped.append({'id':row['id'],\n","                    'values':ast.literal_eval(row['values']),\n","                    'metadata':meta})\n","\n","    #upsert to pinecone every 250 records\n","    if len(prepped) >= 250:\n","        index.upsert(prepped)\n","        prepped = []\n"],"metadata":{"id":"FyAbEZ4LzaEn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["index.describe_index_stats()"],"metadata":{"id":"nnKQcAMjzhKI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#connect to OpenAI\n","OPENAI_API_KEY = utils.get_openai_api_key()\n","openai_client = OpenAI(api_key=OPENAI_API_KEY)\n","\n","#The function returns the embeddings for the given input articles.\n","def get_embeddings(articles, model=\"text-embedding-ada-002\"):\n","   return openai_client.embeddings.create(input = articles, model=model)"],"metadata":{"id":"JquzOf6zzlsF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Run Your Query\n","query = \"what is the berlin wall?\"\n","\n","#communicates with OpenAI's API to get the embedding for the query text.\n","embed = get_embeddings([query])\n","\n","#Queries the Pinecone index using the embedding of the query.\n","\n","#embed.data:The data attribute of the embed object contains the actual response from the OpenAI embeddings API,\n","# this is usually a list of results corresponding to each input provided to the get_embeddings function.\n","\n","#embed.data[0]: Since you provided a single query, embed.data[0] accesses the first (and in this case, the only) item in the response list,\n","# this item is an object representing the embedding of your query.\n","\n","#.embedding: Finally, the embedding attribute of this object contains the actual numerical vector representing the semantic embedding of your query.\n","\n","#metadata of the matches (which includes the actual text) is returned in the response\n","res = index.query(vector=embed.data[0].embedding, top_k=3, include_metadata=True)\n","\n","#Extracts the text from the metadata of each result in res['matches'].\n","text = [r['metadata']['text'] for r in res['matches']]\n","print('\\n'.join(text))\n"],"metadata":{"id":"sEUBuB96zlt3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Build the Prompt\n","\n","#Setting the Query and Getting Embeddings\n","query = \"write an article titled: what is the berlin wall?\"\n","embed = get_embeddings([query])\n","\n","#Querying the Pinecone Index, retrieving the top 3 most similar articles or documents based on their embeddings\n","res = index.query(vector=embed.data[0].embedding, top_k=3, include_metadata=True)\n","\n","#Extracts the text content from each of the top matches' metadata for building prompt\n","contexts = [\n","    x['metadata']['text'] for x in res['matches']\n","]\n","\n","prompt_start = (\n","    \"Answer the question based on the context below.\\n\\n\"+\n","    \"Context:\\n\"\n",")\n","\n","prompt_end = (\n","    f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",")\n","\n","#prompt contains both question and related context\n","prompt = (\n","    prompt_start + \"\\n\\n---\\n\\n\".join(contexts) +\n","    prompt_end\n",")\n","\n","print(prompt)"],"metadata":{"id":"q5pQMetXzlwV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#using OpenAI's GPT-3.5 model to generate a response based on the given prompt\n","res = openai_client.completions.create(\n","    model=\"gpt-3.5-turbo-instruct\",\n","    prompt=prompt,\n","    temperature=0,\n","    max_tokens=636,\n","    top_p=1,\n","\n","    # Adjustments to discourage repetition; both are set to 0 here.\n","    frequency_penalty=0,\n","    presence_penalty=0,\n","\n","    #A sequence where the model should stop generating further content. None means no specific stop sequence.\n","    stop=None\n",")\n","print('-' * 80)\n","\n","#Accesses and prints the generated text from the first choice in the response\n","print(res.choices[0].text)\n","\n","### After running this code, you should see a generated text block that answers the query based on the supplied contexts."],"metadata":{"id":"46A1ddb68ZNA","executionInfo":{"status":"ok","timestamp":1707064027080,"user_tz":360,"elapsed":2,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"D1-IPpPb8dIR"},"execution_count":null,"outputs":[]}]}