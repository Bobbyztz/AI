{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM/thVQKk2A7jFbDYu6oCe1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9_KKF3inRPEq"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","from datasets import load_dataset\n","from pinecone_text.sparse import BM25Encoder\n","from pinecone import Pinecone, ServerlessSpec\n","from DLAIUtils import Utils\n","\n","from sentence_transformers import SentenceTransformer\n","from tqdm.auto import tqdm\n","import torch\n","import os\n","\n","utils = Utils()\n","PINECONE_API_KEY = utils.get_pinecone_api_key()\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(device)\n","\n","utils = Utils()\n","INDEX_NAME = utils.create_dlai_index_name('dl-ai')\n","\n","pinecone = Pinecone(api_key=PINECONE_API_KEY)\n","\n","if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:\n","  pinecone.delete_index(INDEX_NAME)\n","pinecone.create_index(\n","  INDEX_NAME,\n","  dimension=512,\n","  metric=\"dotproduct\",\n","  spec=ServerlessSpec(cloud='aws', region='us-west-2')\n",")\n","index = pinecone.Index(INDEX_NAME)"]},{"cell_type":"code","source":["fashion = load_dataset(\n","    \"ashraq/fashion-product-images-small\",\n","    #only the training portion of the dataset is loaded.\n","    split=\"train\"\n",")\n","fashion"],"metadata":{"id":"DKEAotSdS50o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images = fashion['image']\n","metadata = fashion.remove_columns('image')\n","images[900]"],"metadata":{"id":"2Bdtb0dcS6TP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#....\n","metadata = metadata.to_pandas()\n","metadata.head()"],"metadata":{"id":"jj8dTENkS6WF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Create the Sparse Vector Using BM25\n","#BM25 is a popular information retrieval function that helps in ranking documents based on the query terms appearing in each document.\n","#It's widely used in search engines and recommendation systems for its effectiveness in capturing term importance\n","bm25 = BM25Encoder()\n","bm25.fit(metadata['productDisplayName'])\n","metadata['productDisplayName'][0]\n","\n","bm25.encode_queries(metadata['productDisplayName'][0])\n","bm25.encode_documents(metadata['productDisplayName'][0])"],"metadata":{"id":"Lyn8BagtVAo7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Create the Dense Vector Using CLIP( (Contrastive Languageâ€“Image Pretraining) model)\n","\n","#a variant of the CLIP model, which has been trained to understand both images and text\n","model = SentenceTransformer('sentence-transformers/clip-ViT-B-32',\n","    device=device)\n","\n","#prints the model's details\n","model\n","dense_vec = model.encode([metadata['productDisplayName'][0]])\n","dense_vec.shape\n","\n","len(fashion)"],"metadata":{"id":"SxNVir3BXbK_","executionInfo":{"status":"ok","timestamp":1707087883141,"user_tz":360,"elapsed":2,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["#creating a hybrid index that combines both sparse and dense vector embeddings\n","\n","#Defines the batch size for processing and the total number of data items to be processed.\n","batch_size = 100\n","fashion_data_num = 1000\n","\n","for i in tqdm(range(0, min(fashion_data_num,len(fashion)), batch_size)):\n","    # find end of batch\n","    i_end = min(i+batch_size, len(fashion))\n","    # extract metadata batch\n","    meta_batch = metadata.iloc[i:i_end]\n","    meta_dict = meta_batch.to_dict(orient=\"records\")\n","    # concatinate all metadata field except for id and year to form a single string\n","    meta_batch = [\" \".join(x) for x in meta_batch.loc[:, ~meta_batch.columns.isin(['id', 'year'])].values.tolist()]\n","    # extract image batch\n","    img_batch = images[i:i_end]\n","    # create sparse BM25 vectors\n","    sparse_embeds = bm25.encode_documents([text for text in meta_batch])\n","    # create dense vectors\n","    dense_embeds = model.encode(img_batch).tolist()\n","    # create unique IDs\n","    ids = [str(x) for x in range(i, i_end)]\n","\n","    upserts = []\n","    # loop through the data and create dictionaries for uploading documents to pinecone index\n","    for _id, sparse, dense, meta in zip(ids, sparse_embeds, dense_embeds, meta_dict):\n","        upserts.append({\n","            'id': _id,\n","            'sparse_values': sparse,\n","            'values': dense,\n","            'metadata': meta\n","        })\n","    # upload the documents to the new hybrid index\n","    index.upsert(upserts)\n","\n","# show index description after uploading the documents\n","index.describe_index_stats()"],"metadata":{"id":"HSjdhHSjXdUm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Run Your Query\n"],"metadata":{"id":"u-mgPlJ6axGI"}},{"cell_type":"code","source":["#perform a hybrid search using both sparse and dense vectors to query a Pinecone index\n","#This process effectively leverages both the keyword-based precision of sparse vectors and the semantic understanding of dense vectors\n","\n","query = \"dark blue french connection jeans for men\"\n","\n","#Generating Sparse Vector with BM25\n","sparse = bm25.encode_queries(query)\n","#Utilizes the CLIP model (loaded earlier as model) to encode the query into a dense vector\n","dense = model.encode(query).tolist()\n","\n","#Executes a query on the Pinecone index using both the dense and sparse vectors\n","result = index.query(\n","    top_k=14,\n","    vector=dense,\n","    sparse_vector=sparse,\n","    include_metadata=True\n",")\n","\n","# Iterates through the matches in the query result.\n","# Extracts the image corresponding to each match using the id field from the result. The id here is assumed to be an index or identifier that can be used to locate the corresponding image in the images dataset.\n","# imgs is a list containing the images of the top matching fashion items based on the query.\n","imgs = [images[int(r[\"id\"])] for r in result[\"matches\"]]\n","imgs"],"metadata":{"id":"w1XvzGOsanTv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.core.display import HTML\n","from io import BytesIO\n","from base64 import b64encode\n","\n","# function to display product images in Jupyter Notebook\n","def display_result(image_batch):\n","    figures = []\n","    for img in image_batch:\n","\n","        #Converting Images to Base64-Encoded PNGs\n","        b = BytesIO()\n","        img.save(b, format='png')\n","        figures.append(f'''\n","            <figure style=\"margin: 5px !important;\">\n","              <img src=\"data:image/png;base64,{b64encode(b.getvalue()).decode('utf-8')}\" style=\"width: 90px; height: 120px\" >\n","            </figure>\n","        ''')\n","    return HTML(data=f'''\n","        <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n","        {''.join(figures)}\n","        </div>\n","    ''')"],"metadata":{"id":"Sa98WTXuarWi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display_result(imgs)\n"],"metadata":{"id":"4kpmYuy-at5W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def hybrid_scale(dense, sparse, alpha: float):\n","    \"\"\"Hybrid vector scaling using a convex combination\n","\n","    alpha * dense + (1 - alpha) * sparse\n","\n","    Args:\n","        dense: Array of floats representing\n","        sparse: a dict of `indices` and `values`\n","        alpha: float between 0 and 1 where 0 == sparse only\n","               and 1 == dense only\n","    \"\"\"\n","    if alpha < 0 or alpha > 1:\n","        raise ValueError(\"Alpha must be between 0 and 1\")\n","    # scale sparse and dense vectors to create hybrid search vecs\n","    hsparse = {\n","        'indices': sparse['indices'],\n","        'values':  [v * (1 - alpha) for v in sparse['values']]\n","    }\n","    hdense = [v * alpha for v in dense]\n","    return hdense, hsparse"],"metadata":{"id":"NQ5zMc3eqW1G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#More Dense\n","\n","question = \"dark blue french connection jeans for men\"\n","#Closer to 0==more sparse, closer to 1==more dense\n","hdense, hsparse = hybrid_scale(dense, sparse, alpha=1)\n","result = index.query(\n","    top_k=6,\n","    vector=hdense,\n","    sparse_vector=hsparse,\n","    include_metadata=True\n",")\n","imgs = [images[int(r[\"id\"])] for r in result[\"matches\"]]\n","display_result(imgs)\n","\n","for x in result[\"matches\"]:\n","    print(x[\"metadata\"]['productDisplayName'])"],"metadata":{"id":"ttfpJoOLsG61"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#More Sparse\n","\n","question = \"dark blue french connection jeans for men\"\n","#Closer to 0==more sparse, closer to 1==more dense\n","hdense, hsparse = hybrid_scale(dense, sparse, alpha=0)\n","result = index.query(\n","    top_k=6,\n","    vector=hdense,\n","    sparse_vector=hsparse,\n","    include_metadata=True\n",")\n","imgs = [images[int(r[\"id\"])] for r in result[\"matches\"]]\n","display_result(imgs)\n","\n","for x in result[\"matches\"]:\n","    print(x[\"metadata\"]['productDisplayName'])"],"metadata":{"id":"ON6_DtddsG9s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#More Dense or More Sparse?\n","\n","question = \"dark blue french connection jeans for men\"\n","#Closer to 0==more sparse, closer to 1==more dense\n","hdense, hsparse = hybrid_scale(dense, sparse, alpha=1)\n","result = index.query(\n","    top_k=6,\n","    vector=hdense,\n","    sparse_vector=hsparse,\n","    include_metadata=True\n",")\n","imgs = [images[int(r[\"id\"])] for r in result[\"matches\"]]\n","display_result(imgs)\n","\n","for x in result[\"matches\"]:\n","    print(x[\"metadata\"]['productDisplayName'])\n"],"metadata":{"id":"IKdAjARssHAo"},"execution_count":null,"outputs":[]}]}