{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMG0/ksce6aFA3bnCdlmfmb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"mOY-GqaZC_iC"},"outputs":[],"source":["#fake_chatgpt_scratch"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z80V3tuCDyEo","executionInfo":{"status":"ok","timestamp":1682533250272,"user_tz":300,"elapsed":2193,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}},"outputId":"96cb90af-8d5c-4864-9fd7-ec9f5cff1778"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Colab Notebooks/LLM/fake_chatgpt_scratch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jKW7L4GzEdeh","executionInfo":{"status":"ok","timestamp":1682533250272,"user_tz":300,"elapsed":7,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}},"outputId":"83c52196-3b53-4b20-e503-739ef189b98b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/LLM/fake_chatgpt_scratch\n"]}]},{"cell_type":"code","source":["%pwd #display the current working directory."],"metadata":{"id":"PCreTGhFFrJr","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1682533250272,"user_tz":300,"elapsed":5,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}},"outputId":"4a1f3b89-0344-43d9-8049-48976f585c0d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Colab Notebooks/LLM/fake_chatgpt_scratch'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["import torch #a popular deep learning framework.\n","import numpy as np\n","import requests #used for making HTTP requests.\n","## import tiktoken\n","import torch.nn as nn #Imports the neural network module from PyTorch\n","\n","from torch.nn import functional as F #Imports the functional module from PyTorch's neural network package as 'F'\n","                      #to use various functions like activation and loss functions."],"metadata":{"id":"AndCoGbID8_Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install requests\n","!pip install tiktoken    ## requires python   >    3.9"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zQSRSoJZECdl","executionInfo":{"status":"ok","timestamp":1682533264441,"user_tz":300,"elapsed":11780,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}},"outputId":"ed160ccd-1057-40af-ed8d-f1dbee2deef4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (2.27.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests) (3.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tiktoken in /usr/local/lib/python3.9/dist-packages (0.3.3)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2.27.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2022.10.31)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n"]}]},{"cell_type":"code","source":["############################################################\n","\n","torch.manual_seed(1337)     #Sets the random seed for PyTorch to ensure reproducibility.\n","\n","block_size = 256      ## max content length for predictions\n","batch_size = 64 \n","max_iters  = 5000\n","eval_interval = 500     #Sets the interval for evaluating the model during training.\n","learning_rate = 3e-4             ## 0.001\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","eval_iters = 200      #Sets the number of evaluation iterations.\n","vocab_size = 65     #Sets the size of the vocabulary.\n","n_embd  = 384     #Sets the size of the embedding vectors for each token.(id)\n","n_head  = 6     #Sets the number of attention heads for the Transformer model.\n","n_layer = 6     #Sets the number of layers for the Transformer model.\n","dropout = 0.2     #Sets the dropout rate for the model's layers.\n","\n","############################################################"],"metadata":{"id":"9ngVCD61EOya"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_file_path = '/content/drive/MyDrive/Colab Notebooks/LLM/fake_chatgpt_scratch/input.txt'\n","\n","with open(input_file_path, 'r', encoding='utf-8') as f:\n","    text = f.read()\n","    \n","############################################################\n","\n","print(\"length of data in characters\")\n","len(text)\n","\n","############################################################"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IfyCyYLLERh9","executionInfo":{"status":"ok","timestamp":1682533264441,"user_tz":300,"elapsed":14,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}},"outputId":"8d4b2ca7-55d5-447d-ecc4-b574d1426dcd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["length of data in characters\n"]},{"output_type":"execute_result","data":{"text/plain":["1115393"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["chars = sorted(     list(set(text))   )     #Creates a sorted list of unique characters present in the input text.\n","\n","vocab_size = len(chars)     #Calculates the vocabulary size by counting the number of unique characters.\n","\n","print(  ''.join(chars)  )     #Prints the sorted unique characters as a single string.\n","\n","############################################################# "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uD6qE1MwE3fP","executionInfo":{"status":"ok","timestamp":1682533264442,"user_tz":300,"elapsed":14,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}},"outputId":"dcf0496d-ffaa-48f5-a446-5cf5f1e6cd18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"]}]},{"cell_type":"code","source":["chars = sorted(     list(set(text))   )     #Creates a sorted list of unique characters present in the input text.\n","\n","vocab_size = len(chars)     #Calculates the vocabulary size by counting the number of unique characters.\n","\n","print(  ''.join(chars)  )     #Prints the sorted unique characters as a single string.\n","\n","############################################################# "],"metadata":{"id":"IrsspdhqE6ra"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Encodes the input text as a list of integers, then converts it to a PyTorch tensor of long integers.\n","data = torch.tensor(   encode(text), dtype=torch.long   )  \n","\n","#Calculates the number of data points to be used for training (90% of the data).\n","n    = int(   0.9*len(data)   )\n","\n","#Splits the data tensor into training and validation sets using the calculated value 'n'.\n","train_data = data[:n]\n","val_data   = data[n:]\n","\n","#############################################################"],"metadata":{"id":"WhF0Qhm-E9UD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#for generating batches of data for training or validation.\n","#It takes a single parameter, split, which determines whether the data is for training or validation.\n","def get_batch(split):\n","    if split == \"train\":\n","        data = train_data\n","    else:\n","        data = val_data\n","    \n","    #Generates a 1D tensor of random integers with the shape (batch_size,).\n","    #The random integers are sampled from a range of 0 to len(data) - block_size.\n","    ix = torch.randint(   len(data) - block_size, (batch_size,)   )\n","\n","    #Constructs the input tensor x by extracting slices of length block_size from the data tensor using the random indices from the ix tensor.\n","    #The resulting tensor is stacked along a new dimension.\n","    x  = torch.stack(    [  data[ i : i+block_size ]   for i in ix]    ) \n","\n","    #Constructs the target tensor y in a similar way to x, but with an offset of 1, effectively shifting the data one step forward.\n","    y  = torch.stack(    [  data[ i+1 : i+1+block_size ]   for i in ix]    )\n","    \n","    #Moves the input and target tensors to the specified device (GPU or CPU).\n","    x, y = x.to(device), y.to(device)\n","\n","    return x, y\n","\n","############################################################"],"metadata":{"id":"Gv0ccpzDE_xd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","@torch.no_grad()    ## decorator to disable gradient calculations for efficiency.\n","\n","#estimates the average loss on the training and validation sets without updating the model parameters.\n","def estimate_loss():\n","    out = {}      #empty dictionary to store the average losses for both training and validation sets.\n","    model.eval()   ## no training; Sets the model to evaluation mode, which disables features like dropout and batch normalization.\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)      #Initializes a tensor of zeros with a length equal to the number of evaluation iterations (eval_iters).\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)     #to obtain input-target pairs\n","            logits, loss = model(X, Y)      #to compute the logits and loss for the given input-target pair.\n","            losses[k] = loss.item()     #Assigns the current iteration's loss value to the corresponding index in the losses tensor.\n","        out[split] = losses.mean()      #Calculates the mean of the losses and assigns it to the corresponding key in the out dictionary.\n","    model.train()  ## model back to training mode, re-enabling features like dropout and batch normalization.\n","    return out\n","\n","##########################################################################################"],"metadata":{"id":"skF-jxtfFCFa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#Defines the Head class, which inherits from the nn.Module class in PyTorch.\n","class Head(nn.Module):\n","    \"\"\" one head of self-attention \"\"\"\n","    \n","    def __init__(self, head_size):\n","        super().__init__()\n","\n","        #These lines define the linear transformation layers for the key, query, and value projections.\n","        self.key   = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        \n","        # Creates a lower-triangular mask with ones in the lower triangle and registers it as a buffer. \n","        # This mask is used to enforce the causal attention constraint.\n","        ## the mask tril is not part of the graph since only for masking\n","        ## so register buffer makes it a thing out of the graph\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","        \n","        #Initializes a dropout layer with a dropout rate defined by the dropout variable.\n","        self.dropout = nn.Dropout(dropout)\n","\n","    #Defines the forward pass for the self-attention head.\n","    def forward(self, x):\n","\n","        #Extracts the batch size (B), sequence length (T), and number of channels (C) from the input tensor x.\n","        B, T, C = x.shape\n","\n","        #Applies the key and query linear transformations to the input tensor x.\n","        k = self.key(x)              ## (B, T, C)\n","        q = self.query(x)            ## (B, T, C)\n","        \n","        #Computes the attention weights by taking the dot product of the query and key tensors and\n","        #scaling by the inverse square root of the number of channels (C).\n","        wei = q @ k.transpose(-2, -1) * C**-0.5       ## (B, T, C) @ (B, C, T)  -> (B, T, T)\n","\n","        #Applies the causal attention mask by filling the upper triangle of the attention weights tensor with negative infinity.\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))     ## (B, T, T)\n","\n","        #Computes the softmax of the attention weights along the last dimension.\n","        wei = F.softmax(wei, dim= -1)           ## (B, T, T)\n","\n","        #Applies dropout to the attention weights.\n","        wei = self.dropout(   wei   )\n","        \n","        ## perform the weighted aggregation of the values/Applies the value linear transformation to the input tensor x.\n","        v   = self.value(  x  )   ## (B, T, C)\n","\n","        #Computes the output of the self-attention head\n","        out = wei @ v             ## (B, T, T) @ (B, T, C) -> (B, T, C)\n","        \n","        return out\n","        \n","##########################################################################################"],"metadata":{"id":"QLCJAeGcFFkI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\" multiple heads of self-attention in parallel \"\"\"\n","    \n","    #num_heads, which is the number of attention heads, and head_size, which is the size of each attention head.\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","\n","        #Initializes a list of Head instances using the head_size parameter, and wraps it in an nn.ModuleList for correct handling of submodules in PyTorch.\n","        self.heads = nn.ModuleList(  [Head(head_size) for _ in range(num_heads) ] )\n","\n","        #Defines a linear layer for projecting the concatenated output of the attention heads back to the original input dimensions.\n","        self.proj  = nn.Linear(n_embd, n_embd)\n","\n","        #Initializes a dropout layer with a dropout rate defined by the dropout variable.\n","        self.dropout = nn.Dropout(dropout)\n","    \n","    #Defines the forward pass for the multi-head attention.\n","    def forward(self, x):\n","\n","        #Applies each Head instance in self.heads to the input tensor x and concatenates the outputs along the last dimension.\n","        out = torch.cat(   [ h(x) for h in self.heads], dim = -1   )\n","\n","        #Applies the linear projection layer to the concatenated output, mapping it back to the original input dimensions.\n","        out = self.proj(  out   )\n","\n","        #Applies dropout to the projected output.\n","        out = self.dropout(   out   )\n","\n","        #Returns the output tensor of the multi-head attention.\n","        return out\n","\n","##########################################################################################"],"metadata":{"id":"qbJku5QAFI4b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#feed-forward neural network (FFN) used as a part of the Transformer architecture\n","class FeedForward(nn.Module):\n","    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n","    \n","    #n_embd parameter, which represents the input and output dimensions of the FFN.\n","    def __init__(self, n_embd):\n","        super().__init__()\n","\n","        #Defines the feed-forward network using the nn.Sequential container. The network consists of the following layers:\n","        self.net = nn.Sequential(\n","            #A linear layer that maps the input dimensions to 4 * n_embd.\n","            nn.Linear(n_embd, 4 * n_embd),\n","            #A rectified linear unit (ReLU) activation function.\n","            nn.ReLU(),\n","            #A linear layer that maps the dimensions from 4 * n_embd back to n_embd.\n","            nn.Linear(4 * n_embd, n_embd),\n","            #A dropout layer with a dropout rate defined by the dropout variable.\n","            nn.Dropout(dropout),\n","        )\n","    \n","    #Defines the forward pass for the feed-forward network.\n","    def forward(self, x):\n","        #Applies the self.net container to the input tensor x and returns the output tensor.\n","        return self.net(x)\n","\n","##########################################################################################"],"metadata":{"id":"6bexP-9NFLJE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#The Block class defines a single Transformer block, which consists of a multi-head self-attention mechanism followed by a feed-forward neural network.\n","#The forward pass applies these components in sequence with residual connections and layer normalization.\n","class Block(nn.Module):\n","    \"\"\" Transformer block: comuunication followed by computation \"\"\"\n","    \n","    #n_embd, which represents the input and output dimensions of the block, and n_head, which is the number of self-attention heads in the block.\n","    def __init__(self, n_embd, n_head):\n","        # n_embd: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        #Calculates the size of each attention head by dividing the embedding dimensions n_embd by the number of heads n_head.\n","        head_size = n_embd // n_head\n","        self.sa   = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedForward( n_embd)\n","\n","        #Initializes two layer normalization layers with the specified embedding dimensions.\n","        self.ln1  = nn.LayerNorm(n_embd)\n","        self.ln2  = nn.LayerNorm(n_embd)\n","        \n","    #Defines the forward pass for the Transformer block.\n","    def forward(self, x):\n","        ## these normalizations (ln1, ln2) are about the only thing different from\n","        ## the original Vaswani paper. In the paper, they are done at the end of forward\n","        ## but now they are usually done at the beginning of forward\n","\n","        #Applies the first layer normalization, followed by the multi-head self-attention, and adds the result to the original input x (residual connection).\n","        x = x + self.sa(     self.ln1(x)      )\n","        x = x + self.ffwd(   self.ln2(x)      )\n","        return x\n","    \n","##########################################################################################"],"metadata":{"id":"qQ_jD4JfFNSv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#class BigramLanguageModel, which represents the main language model architecture used in the project. It is a Transformer-based model adapted for the language modeling task.\n","class BigramLanguageModel(nn.Module):\n","    \n","    def __init__(self):\n","        super().__init__()\n","        #Initializes an embedding layer for converting token indices to continuous vectors of dimension n_embd.\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","\n","        #Initializes an embedding layer for positional encoding, which represents the position of tokens in the input sequence.\n","        self.position_embedding_table = nn.Embedding(block_size, n_embd)     ## positional encoding \n","\n","        #Initializes a sequence of Transformer blocks\n","        self.blocks = nn.Sequential(\n","                *[   Block(n_embd, n_head=n_head) for _ in range(n_layer)    ]\n","        )\n","\n","        #a layer normalization layer for the output of the Transformer blocks.\n","        self.ln_f    = nn.LayerNorm(  n_embd    )        ## final layer norm\n","\n","        #Initializes a linear layer for mapping the output embeddings to logits for each token in the vocabulary\n","        self.lm_head = nn.Linear(n_embd, vocab_size)\n","        \n","    \n","    def forward(self, idx, targets=None):\n","        \n","        B, T = idx.shape\n","        \n","        #processes the input idx by applying token embeddings, position embeddings, Transformer blocks, and layer normalization.\n","\n","        ## ids and targets are both (B, T) tensor of integers\n","        tok_emb = self.token_embedding_table(idx)      ## batch, time, embed (4, 8, 32) \n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device))      ## (T, C)\n","        x = tok_emb + pos_emb    ## (B, T, C)\n","        x = self.blocks(  x  )   ## (B, T, C)        \n","        x = self.ln_f(x)         ## (B, T, C)\n","\n","        #maps the output to logits using self.lm_head.\n","        logits = self.lm_head(x)                 ## (B, T, vocab_sice)\n","        \n","        # the model calculates the cross-entropy loss between the logits and targets\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits  = logits.view(B*T, C)\n","            targets  = targets.view(B*T)\n","            loss   = F.cross_entropy(logits, targets)\n","        \n","        return logits, loss\n","    \n","    #takes the initial input token indices idx and generates a sequence of new tokens up to max_new_tokens in length.\n","    def generate(self, idx, max_new_tokens):\n","        \n","        ## idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            \n","            ## crop idx to the last block_size tokens\n","            idx_cond = idx[:, -block_size:]\n","            ## get the predictions\n","            logits, loss = self(idx_cond)\n","            ## focus only on last time stamp\n","            logits = logits[:, -1, :]           ## becomes (B, C)\n","            ## apply softmax to get probs\n","            probs = F.softmax(logits, dim= -1)    ## (B, C)\n","            ## sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1)     ## (B, 1)\n","            ## append sample to the running sequence\n","            idx = torch.cat(  (idx, idx_next), dim=1  )            ## (B, T+1)\n","        return idx\n","            \n","            \n","            \n","######################################################################\n"],"metadata":{"id":"pe1aA0IfFQbQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","model   = BigramLanguageModel()\n","m = model.to(device)\n","\n","######################################################################\n","\n","#Initializes the Adam optimizer with the model's parameters and the specified learning rate.\n","optimizer = torch.optim.Adam(  m.parameters(), lr=learning_rate   )\n","\n","######################################################################\n","#This training loop iteratively trains the BigramLanguageModel using input and target batches,\n","# computes the gradients using backpropagation, and updates the model's parameters using the Adam optimizer.\n","# It also evaluates the model at specified intervals and prints the training and validation losses.\n","\n","for iter in range(max_iters):\n","    if iter % eval_interval == 0:\n","\n","        #calculate the average training and validation losses.\n","        losses = estimate_loss()\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    xb, yb = get_batch('train')\n","    \n","    ## evaluate the loss\n","    logits, loss = m(xb, yb)\n","\n","    #Resets the gradients of the model's parameters to zero before backpropagation.\n","    optimizer.zero_grad(set_to_none=True)   ## zero out\n","\n","    #Computes the gradients of the loss with respect to the model's parameters using backpropagation.\n","    loss.backward()\n","\n","    #Updates the model's parameters based on the calculated gradients and the optimizer's learning rate.\n","    optimizer.step()\n","    \n","\n","################################################################"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kix5dXIPFS3e","outputId":"2a5edfab-08bd-44ac-930a-bca674013bf7","executionInfo":{"status":"ok","timestamp":1682535184151,"user_tz":300,"elapsed":740751,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["step 0: train loss 4.3832, val loss 4.3875\n","step 500: train loss 1.9829, val loss 2.0668\n","step 1000: train loss 1.5972, val loss 1.7695\n","step 1500: train loss 1.4319, val loss 1.6386\n","step 2000: train loss 1.3373, val loss 1.5677\n","step 2500: train loss 1.2738, val loss 1.5253\n","step 3000: train loss 1.2222, val loss 1.4982\n","step 3500: train loss 1.1833, val loss 1.4900\n","step 4000: train loss 1.1455, val loss 1.4854\n","step 4500: train loss 1.1090, val loss 1.5001\n"]}]},{"cell_type":"code","source":["################################################################\n","#### now, regenerate after some training\n","\n","\n","## Kick off generation with some starting token. In this case id 0\n","#generates a sequence of text up to 500 tokens in length, starting from an initial context of all zeros, using the trained language model.\n","\n","#Creates an initial context tensor of shape (1, 1) filled with zeros, with a data type of torch.\n","#long and placed on the device (GPU or CPU). The context serves as a starting point for the text generation process.\n","context = torch.zeros(  (1, 1),  dtype=torch.long, device=device   )\n","\n","#The resulting tensor has the generated token indices, and the first row (index 0) is extracted and converted to a Python list using tolist().\n","gen_text = m.generate(context, max_new_tokens=500)[0].tolist()\n","\n","#Decodes the generated token indices using the decode() function (which converts indices to characters) and prints the generated text as a string.\n","print(  decode(gen_text)   )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7GdVyQrE6h83","executionInfo":{"status":"ok","timestamp":1682535245716,"user_tz":300,"elapsed":10355,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}},"outputId":"16493920-686f-414f-e2e8-ba2a7cec6fb6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","I cannot cold to hear the brace and league,\n","No deless might that your tomb of worse you.\n","\n","Nurse:\n","Suprehead'st Northumberland.\n","\n","JULIET:\n","Come, stamp'st in love her princess!\n","\n","Nurse:\n","Fall'n one keep the work that hide to pain\n","That we did, you no have enter garden.\n","\n","METRCASURE:\n","No; I; My father hope.\n","\n","ROMEO:\n","No common brief.\n","\n","CAPULET:\n","Hart thou weapon\n","What made thee married, lords, thou art eleven\n","wounds. See the submissips, but our services. Come,\n","Event help me rather; and at the wedden day\n","secred \n"]}]}]}