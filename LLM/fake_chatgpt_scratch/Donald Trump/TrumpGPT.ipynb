{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOJFsAowK2FpfP8KJnnE3wO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"mOY-GqaZC_iC","executionInfo":{"status":"ok","timestamp":1682536391452,"user_tz":300,"elapsed":235,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}}},"outputs":[],"source":["#fake_chatgpt_scratch"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z80V3tuCDyEo","executionInfo":{"status":"ok","timestamp":1682536408104,"user_tz":300,"elapsed":15243,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}},"outputId":"23a6eacd-fe36-427d-c4c3-888f695e7332"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Colab Notebooks/LLM/fake_chatgpt_scratch/Donald Trump"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jKW7L4GzEdeh","executionInfo":{"status":"ok","timestamp":1682536453591,"user_tz":300,"elapsed":865,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}},"outputId":"aedc0a0f-1275-4d69-bc56-bcc0a3eda77b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/LLM/fake_chatgpt_scratch/Donald Trump\n"]}]},{"cell_type":"code","source":["%pwd #display the current working directory."],"metadata":{"id":"PCreTGhFFrJr","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1682536528616,"user_tz":300,"elapsed":183,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}},"outputId":"391cf19e-b739-4e51-d696-cc14636c7484"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Colab Notebooks/LLM/fake_chatgpt_scratch/Donald Trump'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["import torch #a popular deep learning framework.\n","import numpy as np\n","import requests #used for making HTTP requests.\n","## import tiktoken\n","import torch.nn as nn #Imports the neural network module from PyTorch\n","\n","from torch.nn import functional as F #Imports the functional module from PyTorch's neural network package as 'F'\n","                      #to use various functions like activation and loss functions."],"metadata":{"id":"AndCoGbID8_Z","executionInfo":{"status":"ok","timestamp":1682536533272,"user_tz":300,"elapsed":4175,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["!pip install requests\n","!pip install tiktoken    ## requires python   >    3.9"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zQSRSoJZECdl","executionInfo":{"status":"ok","timestamp":1682536542172,"user_tz":300,"elapsed":8904,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}},"outputId":"78afdafd-ae1a-4bac-9194-d88703f27797"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (2.27.1)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests) (1.26.15)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tiktoken\n","  Downloading tiktoken-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2.27.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2022.10.31)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n","Installing collected packages: tiktoken\n","Successfully installed tiktoken-0.3.3\n"]}]},{"cell_type":"code","source":["############################################################\n","\n","torch.manual_seed(1337)     #Sets the random seed for PyTorch to ensure reproducibility.\n","\n","block_size = 256      ## max content length for predictions\n","batch_size = 64 \n","max_iters  = 5000\n","eval_interval = 500     #Sets the interval for evaluating the model during training.\n","learning_rate = 3e-4             ## 0.001\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","eval_iters = 200      #Sets the number of evaluation iterations.\n","vocab_size = 65     #Sets the size of the vocabulary.\n","n_embd  = 384     #Sets the size of the embedding vectors for each token.(id)\n","n_head  = 6     #Sets the number of attention heads for the Transformer model.\n","n_layer = 6     #Sets the number of layers for the Transformer model.\n","dropout = 0.2     #Sets the dropout rate for the model's layers.\n","\n","############################################################"],"metadata":{"id":"9ngVCD61EOya","executionInfo":{"status":"ok","timestamp":1682536542536,"user_tz":300,"elapsed":368,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["input_file_path = '/content/drive/MyDrive/Colab Notebooks/LLM/fake_chatgpt_scratch/Donald Trump/input.txt'\n","\n","with open(input_file_path, 'r', encoding='utf-8') as f:\n","    text = f.read()\n","    \n","############################################################\n","\n","print(\"length of data in characters\")\n","len(text)\n","\n","############################################################"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IfyCyYLLERh9","executionInfo":{"status":"ok","timestamp":1682536558858,"user_tz":300,"elapsed":865,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}},"outputId":"8f47e90d-55ff-414b-fc9d-a21b09b1006c"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["length of data in characters\n"]},{"output_type":"execute_result","data":{"text/plain":["896269"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["chars = sorted(     list(set(text))   )     #Creates a sorted list of unique characters present in the input text.\n","\n","vocab_size = len(chars)     #Calculates the vocabulary size by counting the number of unique characters.\n","\n","print(  ''.join(chars)  )     #Prints the sorted unique characters as a single string.\n","\n","############################################################# "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uD6qE1MwE3fP","executionInfo":{"status":"ok","timestamp":1682536582396,"user_tz":300,"elapsed":187,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}},"outputId":"06a86878-a6ec-4201-f669-fe1bf1e1f5bd"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," !\"$%&'(),-./0123456789:;=?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyzé–—‘’“”…\n"]}]},{"cell_type":"code","source":["## tokenizer\n","\n","#Create dictionaries for character-to-index (stoi) and index-to-character (itos) mappings.\n","stoi = { ch:i for i, ch in enumerate(chars) }\n","itos = { i:ch for i, ch in enumerate(chars) }\n","\n","#for encoding a string to a list of integers (encode\n","encode = lambda s: [ stoi[c]          for c in s   ]    ## encoder: string to integer\n","\n","#for decoding a list of integers back to a string (decode), using the dictionaries created above.\n","decode = lambda l: ''.join(   itos[i] for i in l   )    ## decoder: interger to string\n","\n","#############################################################"],"metadata":{"id":"IrsspdhqE6ra","executionInfo":{"status":"ok","timestamp":1682536582914,"user_tz":300,"elapsed":326,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["#Encodes the input text as a list of integers, then converts it to a PyTorch tensor of long integers.\n","data = torch.tensor(   encode(text), dtype=torch.long   )  \n","\n","#Calculates the number of data points to be used for training (90% of the data).\n","n    = int(   0.9*len(data)   )\n","\n","#Splits the data tensor into training and validation sets using the calculated value 'n'.\n","train_data = data[:n]\n","val_data   = data[n:]\n","\n","#############################################################"],"metadata":{"id":"WhF0Qhm-E9UD","executionInfo":{"status":"ok","timestamp":1682536582914,"user_tz":300,"elapsed":5,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["#for generating batches of data for training or validation.\n","#It takes a single parameter, split, which determines whether the data is for training or validation.\n","def get_batch(split):\n","    if split == \"train\":\n","        data = train_data\n","    else:\n","        data = val_data\n","    \n","    #Generates a 1D tensor of random integers with the shape (batch_size,).\n","    #The random integers are sampled from a range of 0 to len(data) - block_size.\n","    ix = torch.randint(   len(data) - block_size, (batch_size,)   )\n","\n","    #Constructs the input tensor x by extracting slices of length block_size from the data tensor using the random indices from the ix tensor.\n","    #The resulting tensor is stacked along a new dimension.\n","    x  = torch.stack(    [  data[ i : i+block_size ]   for i in ix]    ) \n","\n","    #Constructs the target tensor y in a similar way to x, but with an offset of 1, effectively shifting the data one step forward.\n","    y  = torch.stack(    [  data[ i+1 : i+1+block_size ]   for i in ix]    )\n","    \n","    #Moves the input and target tensors to the specified device (GPU or CPU).\n","    x, y = x.to(device), y.to(device)\n","\n","    return x, y\n","\n","############################################################"],"metadata":{"id":"Gv0ccpzDE_xd","executionInfo":{"status":"ok","timestamp":1682536582915,"user_tz":300,"elapsed":5,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["\n","@torch.no_grad()    ## decorator to disable gradient calculations for efficiency.\n","\n","#estimates the average loss on the training and validation sets without updating the model parameters.\n","def estimate_loss():\n","    out = {}      #empty dictionary to store the average losses for both training and validation sets.\n","    model.eval()   ## no training; Sets the model to evaluation mode, which disables features like dropout and batch normalization.\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)      #Initializes a tensor of zeros with a length equal to the number of evaluation iterations (eval_iters).\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)     #to obtain input-target pairs\n","            logits, loss = model(X, Y)      #to compute the logits and loss for the given input-target pair.\n","            losses[k] = loss.item()     #Assigns the current iteration's loss value to the corresponding index in the losses tensor.\n","        out[split] = losses.mean()      #Calculates the mean of the losses and assigns it to the corresponding key in the out dictionary.\n","    model.train()  ## model back to training mode, re-enabling features like dropout and batch normalization.\n","    return out\n","\n","##########################################################################################"],"metadata":{"id":"skF-jxtfFCFa","executionInfo":{"status":"ok","timestamp":1682536582915,"user_tz":300,"elapsed":5,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["\n","#Defines the Head class, which inherits from the nn.Module class in PyTorch.\n","class Head(nn.Module):\n","    \"\"\" one head of self-attention \"\"\"\n","    \n","    def __init__(self, head_size):\n","        super().__init__()\n","\n","        #These lines define the linear transformation layers for the key, query, and value projections.\n","        self.key   = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        \n","        # Creates a lower-triangular mask with ones in the lower triangle and registers it as a buffer. \n","        # This mask is used to enforce the causal attention constraint.\n","        ## the mask tril is not part of the graph since only for masking\n","        ## so register buffer makes it a thing out of the graph\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","        \n","        #Initializes a dropout layer with a dropout rate defined by the dropout variable.\n","        self.dropout = nn.Dropout(dropout)\n","\n","    #Defines the forward pass for the self-attention head.\n","    def forward(self, x):\n","\n","        #Extracts the batch size (B), sequence length (T), and number of channels (C) from the input tensor x.\n","        B, T, C = x.shape\n","\n","        #Applies the key and query linear transformations to the input tensor x.\n","        k = self.key(x)              ## (B, T, C)\n","        q = self.query(x)            ## (B, T, C)\n","        \n","        #Computes the attention weights by taking the dot product of the query and key tensors and\n","        #scaling by the inverse square root of the number of channels (C).\n","        wei = q @ k.transpose(-2, -1) * C**-0.5       ## (B, T, C) @ (B, C, T)  -> (B, T, T)\n","\n","        #Applies the causal attention mask by filling the upper triangle of the attention weights tensor with negative infinity.\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))     ## (B, T, T)\n","\n","        #Computes the softmax of the attention weights along the last dimension.\n","        wei = F.softmax(wei, dim= -1)           ## (B, T, T)\n","\n","        #Applies dropout to the attention weights.\n","        wei = self.dropout(   wei   )\n","        \n","        ## perform the weighted aggregation of the values/Applies the value linear transformation to the input tensor x.\n","        v   = self.value(  x  )   ## (B, T, C)\n","\n","        #Computes the output of the self-attention head\n","        out = wei @ v             ## (B, T, T) @ (B, T, C) -> (B, T, C)\n","        \n","        return out\n","        \n","##########################################################################################"],"metadata":{"id":"QLCJAeGcFFkI","executionInfo":{"status":"ok","timestamp":1682536582915,"user_tz":300,"elapsed":5,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\" multiple heads of self-attention in parallel \"\"\"\n","    \n","    #num_heads, which is the number of attention heads, and head_size, which is the size of each attention head.\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","\n","        #Initializes a list of Head instances using the head_size parameter, and wraps it in an nn.ModuleList for correct handling of submodules in PyTorch.\n","        self.heads = nn.ModuleList(  [Head(head_size) for _ in range(num_heads) ] )\n","\n","        #Defines a linear layer for projecting the concatenated output of the attention heads back to the original input dimensions.\n","        self.proj  = nn.Linear(n_embd, n_embd)\n","\n","        #Initializes a dropout layer with a dropout rate defined by the dropout variable.\n","        self.dropout = nn.Dropout(dropout)\n","    \n","    #Defines the forward pass for the multi-head attention.\n","    def forward(self, x):\n","\n","        #Applies each Head instance in self.heads to the input tensor x and concatenates the outputs along the last dimension.\n","        out = torch.cat(   [ h(x) for h in self.heads], dim = -1   )\n","\n","        #Applies the linear projection layer to the concatenated output, mapping it back to the original input dimensions.\n","        out = self.proj(  out   )\n","\n","        #Applies dropout to the projected output.\n","        out = self.dropout(   out   )\n","\n","        #Returns the output tensor of the multi-head attention.\n","        return out\n","\n","##########################################################################################"],"metadata":{"id":"qbJku5QAFI4b","executionInfo":{"status":"ok","timestamp":1682536582915,"user_tz":300,"elapsed":5,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["#feed-forward neural network (FFN) used as a part of the Transformer architecture\n","class FeedForward(nn.Module):\n","    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n","    \n","    #n_embd parameter, which represents the input and output dimensions of the FFN.\n","    def __init__(self, n_embd):\n","        super().__init__()\n","\n","        #Defines the feed-forward network using the nn.Sequential container. The network consists of the following layers:\n","        self.net = nn.Sequential(\n","            #A linear layer that maps the input dimensions to 4 * n_embd.\n","            nn.Linear(n_embd, 4 * n_embd),\n","            #A rectified linear unit (ReLU) activation function.\n","            nn.ReLU(),\n","            #A linear layer that maps the dimensions from 4 * n_embd back to n_embd.\n","            nn.Linear(4 * n_embd, n_embd),\n","            #A dropout layer with a dropout rate defined by the dropout variable.\n","            nn.Dropout(dropout),\n","        )\n","    \n","    #Defines the forward pass for the feed-forward network.\n","    def forward(self, x):\n","        #Applies the self.net container to the input tensor x and returns the output tensor.\n","        return self.net(x)\n","\n","##########################################################################################"],"metadata":{"id":"6bexP-9NFLJE","executionInfo":{"status":"ok","timestamp":1682536582915,"user_tz":300,"elapsed":4,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["\n","class Block(nn.Module):\n","    \"\"\" Transformer block: comuunication followed by computation \"\"\"\n","    \n","    def __init__(self, n_embd, n_head):\n","        # n_embd: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.sa   = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedForward( n_embd)\n","        self.ln1  = nn.LayerNorm(n_embd)\n","        self.ln2  = nn.LayerNorm(n_embd)\n","        \n","    def forward(self, x):\n","        ## these normalizations (ln1, ln2) are about the only thing different from\n","        ## the original Vaswani paper. In the paper, they are done at the end of forward\n","        ## but now they are usually done at the beginning of forward\n","        x = x + self.sa(     self.ln1(x)      )\n","        x = x + self.ffwd(   self.ln2(x)      )\n","        return x\n","    \n","##########################################################################################"],"metadata":{"id":"qQ_jD4JfFNSv","executionInfo":{"status":"ok","timestamp":1682536582916,"user_tz":300,"elapsed":5,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["\n","\n","class BigramLanguageModel(nn.Module):\n","    \n","    def __init__(self):\n","        super().__init__()\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embd)     ## positional encoding \n","        self.blocks = nn.Sequential(\n","                *[   Block(n_embd, n_head=n_head) for _ in range(n_layer)    ]\n","        )\n","        self.ln_f    = nn.LayerNorm(  n_embd    )        ## final layer norm\n","        self.lm_head = nn.Linear(n_embd, vocab_size)\n","        \n","    \n","    def forward(self, idx, targets=None):\n","        \n","        B, T = idx.shape\n","        \n","        ## ids and targets are both (B, T) tensor of integers\n","        tok_emb = self.token_embedding_table(idx)      ## batch, time, embed (4, 8, 32) \n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device))      ## (T, C)\n","        x = tok_emb + pos_emb    ## (B, T, C)\n","        x = self.blocks(  x  )   ## (B, T, C)        \n","        x = self.ln_f(x)         ## (B, T, C)\n","        logits = self.lm_head(x)                 ## (B, T, vocab_sice)\n","        \n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits  = logits.view(B*T, C)\n","            targets  = targets.view(B*T)\n","            loss   = F.cross_entropy(logits, targets)\n","        \n","        return logits, loss\n","    \n","    def generate(self, idx, max_new_tokens):\n","        \n","        ## idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            \n","            ## crop idx to the last block_size tokens\n","            idx_cond = idx[:, -block_size:]\n","            ## get the predictions\n","            logits, loss = self(idx_cond)\n","            ## focus only on last time stamp\n","            logits = logits[:, -1, :]           ## becomes (B, C)\n","            ## apply softmax to get probs\n","            probs = F.softmax(logits, dim= -1)    ## (B, C)\n","            ## sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1)     ## (B, 1)\n","            ## append sample to the running sequence\n","            idx = torch.cat(  (idx, idx_next), dim=1  )            ## (B, T+1)\n","        return idx\n","            \n","            \n","            \n","######################################################################\n"],"metadata":{"id":"pe1aA0IfFQbQ","executionInfo":{"status":"ok","timestamp":1682536582916,"user_tz":300,"elapsed":5,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["\n","\n","model   = BigramLanguageModel()\n","m = model.to(device)\n","\n","######################################################################\n","\n","optimizer = torch.optim.Adam(  m.parameters(), lr=learning_rate   )\n","\n","######################################################################\n","\n","for iter in range(max_iters):\n","    if iter % eval_interval == 0:\n","        losses = estimate_loss()\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    xb, yb = get_batch('train')\n","    \n","    ## evaluate the loss\n","    logits, loss = m(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)   ## zero out\n","    loss.backward()\n","    optimizer.step()\n","    \n","\n","################################################################"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kix5dXIPFS3e","outputId":"fd5f1255-c178-4564-bfcd-8090f8aa7b5c","executionInfo":{"status":"ok","timestamp":1682537508710,"user_tz":300,"elapsed":925799,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}}},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["step 0: train loss 4.7092, val loss 4.6695\n","step 500: train loss 1.8405, val loss 3.7560\n","step 1000: train loss 1.3263, val loss 2.8281\n","step 1500: train loss 1.1441, val loss 2.6464\n","step 2000: train loss 1.0420, val loss 2.7472\n","step 2500: train loss 0.9609, val loss 2.5894\n","step 3000: train loss 0.8950, val loss 2.6118\n","step 3500: train loss 0.8370, val loss 2.6907\n","step 4000: train loss 0.7881, val loss 2.7162\n","step 4500: train loss 0.7414, val loss 2.6951\n"]}]},{"cell_type":"code","source":["################################################################\n","#### now, regenerate after some training\n","\n","\n","## Kick off generation with some starting token. In this case id 0\n","\n","context = torch.zeros(  (1, 1),  dtype=torch.long, device=device   )\n","\n","gen_text = m.generate(context, max_new_tokens=500)[0].tolist()\n","\n","print(  decode(gen_text)   )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7GdVyQrE6h83","executionInfo":{"status":"ok","timestamp":1682537518873,"user_tz":300,"elapsed":10172,"user":{"displayName":"Tianzong Zhang","userId":"18141008230024928237"}},"outputId":"532dcc99-f9ae-46fc-a9b0-30bf86408886"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","somebody says, it and it all these political persons who’ve for one for Israel for proper until Islam Hillary does.\n","And what’s happening, I’m the only biggest interest that are many thinking now that. So I’m talking all of the problems, it’s — they’ve had throwing to be a total change.\n","So again, you saw about [ridiculations in people in the Sopeling 175 were into the United States. Donald Everything’s been done is beautiful Islamic terrorism.\n","\n","\n","You know the liberal day deal is a wall. My vote fo\n"]}]}]}