{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eoNhUvLmNvSz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Single head of self-attention.\n",
        "    This class implements a single attention head as part of a larger attention mechanism.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_embd, head_size, dropout, block_size):\n",
        "        \"\"\"\n",
        "        Initializes the SelfAttention layer.\n",
        "        :param n_embd: Size of each embedding vector.\n",
        "        :param head_size: Size of each attention head.\n",
        "        :param dropout: Dropout rate for regularization.\n",
        "        :param block_size: Size of the block for the attention mask.\n",
        "        \"\"\"\n",
        "        super().__init__()  # Initialize the superclass (nn.Module)\n",
        "\n",
        "        # Linear transformations for the keys, queries, and values.\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)  # Linear layer for keys.\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)  # Linear layer for queries.\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)  # Linear layer for values.\n",
        "\n",
        "        # Register a lower triangular matrix as a buffer, used for creating masks in some attention mechanisms.\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        # Dropout layer for regularization.\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the self-attention layer.\n",
        "        :param x: Input tensor of shape (batch size, sequence length, embedding dimension).\n",
        "        \"\"\"\n",
        "        B, T, C = x.shape  # B: Batch size, T: Sequence length, C: Embedding dimension\n",
        "\n",
        "        # Applying linear transformations to compute keys, queries, and values.\n",
        "        k = self.key(x)  # Transform input to keys.\n",
        "        q = self.query(x)  # Transform input to queries.\n",
        "\n",
        "        # Calculate the attention weights.\n",
        "        # q @ k: Perform batch matrix multiplication between queries and keys.\n",
        "        # Transpose the last two dimensions of k for proper matrix multiplication.\n",
        "        # Scale the result by the inverse square root of the dimension of the keys.\n",
        "        weight = q @ k.transpose(-2, -1) * k.shape[-1] ** (-0.5)\n",
        "\n",
        "        # Apply softmax to get probabilities, ensuring the sum of weights for each query is 1.\n",
        "        weight = F.softmax(weight, dim=-1)\n",
        "\n",
        "        # Transform input to values.\n",
        "        v = self.value(x)\n",
        "\n",
        "        # Calculate the output by weighted sum of values.\n",
        "        out = weight @ v  # Batch matrix multiplication between weights and values.\n",
        "\n",
        "        return out  # Return the result of the self-attention mechanism.\n"
      ],
      "metadata": {
        "id": "GNLNuTcWQBU9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F9zmHmp5Q5TP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}